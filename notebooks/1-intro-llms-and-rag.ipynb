{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Solutions with LLMs and Retrieval Augmented Generation (RAG): Introduction\n",
    "\n",
    "Welcome to this AI workshop! In this workshop we will get deeper into AI, RAG, chatbots, embeddings, evaluations and more.\n",
    "But first lets get used to this notebook enviroment.\n",
    "\n",
    "### Getting started with Jupyter Notebooks\n",
    "Notebooks are essential tools in the toolkit of data scientists and machine learning engineers. \n",
    "They facilitate interactive coding and effective visualization of results, making them invaluable for data exploration and analysis.\n",
    "Jupyter allows you to create cells that can contain either text or code. \n",
    "You can execute code cells by pressing Ctrl + Enter, enabling you to run your code and see results instantly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Run the cell bellow a couple of times to get used to the notebook behaviour.\n",
    "2. Restart the kernel (Restart) button above and run again this code to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a if it does not exist\n",
    "if  'a' not in globals():\n",
    "    a = 42\n",
    "\n",
    "a = a+1\n",
    "\n",
    "# press ctrl+enter to run the cell, do that multiple times\n",
    "# notice how the variable `a` is persistent in the cell\n",
    "# you can run full programs in a notebook.\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LLMs\n",
    "LLMs, or Large Language Models, are advanced AI systems designed to understand and generate human language. They use deep learning techniques and are trained on vast amounts of text data. Key features include their large scale (billions of parameters), ability to perform various language tasks (like translation and summarization), and contextual understanding. \n",
    "### MistralAI\n",
    "\n",
    "For this workshop we wil use [Mistral AI](https://mistral.ai/) langauge models, which are similar in concept to OpenAI's ChatGPT and Anthropic's Claude. Mistral AI offers both premier models and free models, with a strong emphasis on open-source availability. We are going to use [Mistral Small](https://docs.mistral.ai/getting-started/models/models_overview/#premier-models) model today. \n",
    "\n",
    "If your installation finalized correctly you should have mistral already installed. \n",
    "The command below checks if mistral is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mistralai\n",
      "Version: 1.1.0\n",
      "Summary: Python Client SDK for the Mistral AI API.\n",
      "Home-page: https://github.com/mistralai/client-python.git\n",
      "Author: Mistral\n",
      "Author-email: \n",
      "License: \n",
      "Location: /workspaces/ai-rag-quiz-workshop/.virtualenvironment/lib/python3.12/site-packages\n",
      "Requires: eval-type-backport, httpx, jsonpath-python, pydantic, python-dateutil, typing-inspect\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason mistral did not installed successfully you can get it (and all other dependencies of this workshop by installing it directly with the command below), just uncomment the lines which start with \"!pip install\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this command will install all the dependencies of the requirements.txt file\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "# this command will install the code of the project itsel\n",
    "# !pip install -e ../\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting access to Mistral LLMs\n",
    "\n",
    "LLMs are large in size and use a lot of computing resources. The most common way to use them is to run them in the cloud via network calls. For the sake of this workshop, we will use the cloud version as well, so we don't need to download large models. If you are interested in running LLMs locally, check out [ollama](https://github.com/ollama/ollama), arguably the most advanced project that does this.\n",
    "\n",
    "\n",
    "#### API key\n",
    "The second step is to get a Mistral API key. You can find some API keys we prepared for this workshop in this [sheet](https://docs.google.com/spreadsheets/d/1ZwTpkG6OOuVrOx8nzPmgai_7Hwpo8Kun7yZrOmg_5K4/edit?gid=0#gid=0). Get the key (please write your name next to it in the sheet so that people know it is taken) and write it to the .env file using the command below in Task 2.\n",
    "\n",
    "If you want to get your own API key, you can do it on the [website here](https://auth.mistral.ai/ui/registration).You will need to sign up using your email address and phone number and create a new API key [here]((https://console.mistral.ai/api-keys/)). If you are having any trouble, check out the video instructions on how to do it [here](https://drive.google.com/file/d/1mqwkX1BRvg_RMZJQHjtvKq31RjWZ9MdK/view)\n",
    "\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Use the cell below to write your API key into the file so you can use Mistral. We write the variable into a file named .env. The .env files are a standard in Python to load information like API keys and passwords that should not stay with the code for security reasons. **Make sure to delete your key from the cell once you write it to the .env file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file .env\n",
    "\n",
    "mistral_api_key = 'gtaba0LLreCD6dfa02aQtBFzYUc1zHww'\n",
    "with open('../.env', 'w') as f:\n",
    "    f.write(f'MISTRAL_API_KEY=\"{mistral_api_key}\"')\n",
    "\n",
    "# make sure to delete the API key from this cell before commiting the file so you dont save your key in the repo which is a security risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if it worked\n",
    "\n",
    "Run the command below to check if writing the key worked.  It will trigger an error if it did not.  You might need to restart the kernel if it does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /workspaces/ai-rag-quiz-workshop/.env\n",
      "If you reached this point is because it wrote a key in the right place :)\n"
     ]
    }
   ],
   "source": [
    "# we now reload the the configuration file and it should show your key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "env_file = find_dotenv()\n",
    "print(f\"Loading environment variables from {env_file}\")\n",
    "load_dotenv(env_file)\n",
    "\n",
    "import os\n",
    "env = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "if not env:\n",
    "    raise ValueError(\"The API key is not set. Please set the MISTRAL_API_KEY environment variable.\")\n",
    "if env == 'REPLACE WITH YOUR KEY':\n",
    "    raise ValueError(\"You did not repalce the value with the real key\")\n",
    "print(\"If you reached this point is because it wrote a key in the right place :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mistral\n",
    "Let's run the code below to import Mistral and initialize the Mistral client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to tell you a story! Let's dive into a tale called \"The Whispering Woods.\"\n",
      "\n",
      "Once upon a time, in a small village nestled between rolling hills and a sparkling river, there lived a young girl named Elara. Elara was known throughout the village for her curiosity and her love for the woods that surrounded their home. She would often spend her days exploring the woods, learning about the plants and animals that lived there.\n",
      "\n",
      "One day, while wandering deeper into the woods than she ever had before, Elara stumbled upon a hidden glade. The glade was filled with tall, ancient trees that seemed to whisper secrets to one another. In the center of the glade, there was a large, gnarled tree with a face carved into its trunk. The face had kind eyes and a gentle smile, and it seemed to beckon Elara closer.\n",
      "\n",
      "As Elara approached the tree, she heard a soft voice whispering in the wind. \"Greetings, Elara,\" the voice said. \"I am the Spirit of the Woods. I have been waiting for someone like you to come along.\"\n",
      "\n",
      "Elara was startled but not afraid. \"What do you mean?\" she asked.\n",
      "\n",
      "\"The woods are in danger,\" the Spirit explained. \"A dark force is spreading through the forest, turning the plants and animals against one another. I need your help to stop it.\"\n",
      "\n",
      "Elara's heart swelled with determination. \"I'll do whatever I can to help,\" she said.\n",
      "\n",
      "The Spirit of the Woods smiled. \"Good. You must find the three sacred artifacts hidden deep within the woods. They are the only things that can stop the dark force. But be warned, the path will not be easy.\"\n",
      "\n",
      "And so, Elara set out on her quest. She faced many challenges along the way, from navigating treacherous paths to outsmarting mischievous creatures. But with each artifact she found, she felt a surge of hope and strength.\n",
      "\n",
      "Finally, after many days of searching, Elara found the last artifact. She rushed back to the glade, where the Spirit of the Woods awaited her. With the artifacts in hand, Elara followed the Spirit's instructions and used them to banish the dark force from the woods.\n",
      "\n",
      "As the darkness lifted, the woods began to heal. The plants and animals returned to their peaceful ways, and the woods were once again filled with the sound of laughter and song.\n",
      "\n",
      "From that day forward, Elara was known throughout the village as a hero. And whenever she ventured into the woods, she could hear the trees whispering her name, thanking her for saving their home.\n",
      "\n",
      "And so, the tale of Elara and the Whispering Woods became a beloved story, passed down from generation to generation, reminding everyone of the power of courage and kindness.\n",
      "\n",
      "The end.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Retrieve the Mistral API key from the environment variables\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "# The model below is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\"\n",
    "\n",
    "# The code below defines a function `call_mistral_model` that sends a message to a Mistral model and returns the model's response text.\n",
    "response = mistral_client.chat.complete(\n",
    "    model = model_name,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a story\",\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "# Extract only the text from the response\n",
    "response_text = response.choices[0].message.content\n",
    "print(response_text)\n",
    "\n",
    "## not how the result resembles natural language. Its the exact same concept as chatgpt.\n",
    "## feel free to play around with the prompt and see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having all of this complex code to call the LLM, we can simplify it by moving this complexity to a Python script. From the code below onwards, we will replace our calls to the Mistral API with calls to the LargeLanguageModel class.\n",
    "\n",
    "Try it out and take a look at the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.llm import LargeLanguageModel\n",
    "\n",
    "# Initialize an instance of the LargeLanguageModel class\n",
    "llm = LargeLanguageModel()\n",
    "# Make a call to Mistral using the LargeLanguageModel class\n",
    "response = llm.call(\"hello! What is your name?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond calling the LLM, the class contains rate limiting handling logic. Rate limiting is a mechanism implemented by APIs to control the number of requests a user can make in a given time frame. This is done to prevent abuse so that one user doesn't consume the entire capacity and slow down the service. However, rate limiting can be annoying because it can interrupt your workflow and force you to wait before making more requests.\n",
    "\n",
    "To make things easier, we've implemented a rate limit error controller in the LargeLanguageModel class (see usage example below) that automatically adds sleep intervals between requests to avoid exceeding the rate limit. We will use the LargeLanguageModel class moving forward to make calls to Mistral AI. This class has the same logic as the examples we showed above but includes a mechanism to counter the rate limiting issue.    \n",
    "\n",
    "Have a look at the class by Ctrl+clicking on the LargeLanguageModel name to jump directly to its implementation.\n",
    "\n",
    "## Exploring the LLM\n",
    "Now that we have seen how to make a basic call to the Mistral model using the [LargeLanguageModel](../chat_solution/llm.py) class, let's try some more prompts to see how the model responds to different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Modifying Prompts\n",
    "Below are some example use cases of how to use an LLM such as Mistral. Play around with the prompts and see the results. Modify the prompts to see how the model's responses change. This will help you understand how to craft effective prompts and get the desired output from the model.\n",
    "\n",
    "Try to:\n",
    "- Ask different types of questions\n",
    "- Change the text for summarization or extraction (see examples 2 and 3 below)\n",
    "- Alter the style of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Asking for Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! 42Berlin is a coding school that follows the innovative educational model pioneered by 42, a coding school founded in Paris by Xavier Niel, Florian Bucher, Nicolas Sadirac, and Kwame Yamgnane. The school is designed to provide an alternative pathway to learning software development and technology skills, focusing on practical, hands-on experience rather than traditional lectures and exams.\n",
      "\n",
      "### Key Features of 42Berlin:\n",
      "\n",
      "1. **Peer-to-Peer Learning**:\n",
      "   - Students learn primarily through collaboration and peer-to-peer interaction. The school emphasizes teamwork and mutual support.\n",
      "\n",
      "2. **Project-Based Curriculum**:\n",
      "   - The curriculum is centered around practical projects that students work on individually or in groups. This approach helps students apply theoretical knowledge to real-world problems.\n",
      "\n",
      "3. **No Tuition Fees**:\n",
      "   - 42Berlin is tuition-free, making it accessible to a broader range of students who might not otherwise be able to afford traditional education.\n",
      "\n",
      "4. **24/7 Access**:\n",
      "   - The school is open 24 hours a day, 7 days a week, allowing students to learn at their own pace and on their own schedule.\n",
      "\n",
      "5. **Gamified Learning**:\n",
      "   - The learning process is gamified, with students progressing through levels and challenges, similar to a video game. This approach keeps students engaged and motivated.\n",
      "\n",
      "6. **Diverse Student Body**:\n",
      "   - 42Berlin welcomes students from diverse backgrounds, including those without prior formal education in computer science. The school believes in the potential of individuals from all walks of life.\n",
      "\n",
      "7. **Industry Partnerships**:\n",
      "   - The school often collaborates with tech companies, providing students with opportunities for internships and networking, which can lead to job placements after graduation.\n",
      "\n",
      "8. **Global Network**:\n",
      "   - As part of the 42 network, students have access to a global community of learners and alumni, which can be beneficial for networking and career opportunities.\n",
      "\n",
      "### Admission Process:\n",
      "\n",
      "- **Piscine (Pool)**:\n",
      "  - The admission process typically involves a rigorous, multi-week selection process called the \"Piscine.\" During this period, applicants are immersed in an intensive coding bootcamp to assess their problem-solving skills, motivation, and ability to work in a team.\n",
      "\n",
      "### Outcomes:\n",
      "\n",
      "- Graduates of 42Berlin often go on to work in various tech roles, including software development, data science, cybersecurity, and more. The school's focus on practical skills and real-world projects helps graduates to be well-prepared for the demands of the tech industry.\n",
      "\n",
      "### Conclusion:\n",
      "\n",
      "42Berlin offers a unique and innovative approach to learning software development, emphasizing practical skills, peer collaboration, and accessibility. It is an excellent option for those looking for an alternative to traditional education and who are motivated to learn through hands-on experience.\n"
     ]
    }
   ],
   "source": [
    "# Example: Asking for information\n",
    "prompt = \"Can you tell me about coding school 42Berlin?\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Summarizing a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42Berlin is a tuition-free coding school empowering the next generation of coders through accessible and inclusive tech education.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_summarize = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Summarize the following text in one brief sentence: {text_to_summarize}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Extracting Information from a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year 42Berlin was founded is 2021.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_extract_from = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Extract the year 42Berlin was founded from the following text: {text_to_extract_from}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 3: Doing Maths -> When will the LLM make a mistake?\n",
    "\n",
    "LLMs are trained on text language so they are not necessarily good with maths. We can be confident that it will fail to produce the correct result if you ask a question that is complex enough. On the notebook below we increase the complexity of the task at every run. Run it until you see it diverging.\n",
    "\n",
    "What was the number of iterations necessary to make it break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct result:  11398895185373143\n",
      "Mistral result:  The result of 7 raised to the power of 19 (7^19) is a very large number. Here it is:\n",
      "\n",
      "7^19 = 40353607130363288641\n",
      "\n",
      "This number has 19 digits.\n"
     ]
    }
   ],
   "source": [
    "if 'a' not in globals() or 'b' not in globals():\n",
    "    a = 1\n",
    "    b = 1\n",
    "a = a + 1\n",
    "b = b + 3\n",
    "\n",
    "## ** in python is the power operator meaning: a to the power of b\n",
    "print(\"Correct result: \", a**b)\n",
    "response = llm.call(f\"what is the results of {a} ** {b}\")\n",
    "print(\"Mistral result: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably noted, mistral does not only return the results, it also explains it step by step. For example:\n",
    "\n",
    "```\n",
    "The result of 4 raised to the power of 4 (4 ** 4) is calculated as:\n",
    "4 * 4 * 4 * 4 = 256\n",
    "```\n",
    "\n",
    "Mistral and other language models deliberatelly add this longer explanations as they help the model commit less mistakes. This pattern is also known as chain of thought reasonsing. And is one of the most robust ways to improve LLM's responses. When you write prompts think about how to explain the steps of the reasonsing to improve the performance of your propmts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination\n",
    "LLMs sometimes generate responses that are plausible-sounding but factually incorrect or nonsensical. This phenomenon is known as \"hallucination\".\n",
    "<br><br>\n",
    "***Hallucination*** can occur because the model generates text based on patterns in the training data rather than actual knowledge or retrieval of relevant information.\n",
    "LLMs will produce the most likelly words that they would find in similar text, they have no clue about fact vs fiction. They can confidently produce writing about things they have no clue about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Demonstrating Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will ask the model a question that it might to hallucinate an answer for, showing the limitations of relying solely on language generation without retrieval.\n",
    "\n",
    "Try running the command below a few times in a row and see how the response by the LLM changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response likely to hallucinate:\n",
      "\n",
      "The workshop launched by the MLOps Community Berlin in collaboration with 42 Berlin was called \"MLOps Workshop.\" This event aimed to provide participants with hands-on experience and insights into the practices and tools used in the field of Machine Learning Operations (MLOps).\n"
     ]
    }
   ],
   "source": [
    "# Ask a question likely to cause hallucination\n",
    "prompt = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with 42 Berlin?\"\n",
    "response = llm.call(prompt)\n",
    "print(\"Response likely to hallucinate:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to the next section on Retrieval-Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that incorporates external information to a LLM to generate better responses. \n",
    "\n",
    "In RAG, a retrieval component searches and retrieves relevant information from a knowledge base or external documents, and a generation component uses this information to generate responses.\n",
    "This approach allows the model to access up-to-date information and provide more detailed and accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will demonstrate a simple example of how to use Retrieval-Augmented Generation. We will use a predefined set of documents, retrieve relevant information based on a query, and then generate a response using the retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message: str, context: str):\n",
    "    \"\"\"\n",
    "    Message is the question that the user is asking.\n",
    "    Context is the information that we want to use to answer the question.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code in the cell below, you can compare how our LLM responses differ by the information that you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Women.\" This workshop aimed to empower women in the field of machine learning operations (MLOps) by providing them with practical skills, knowledge, and networking opportunities.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " The name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech is \"AI Launchpad - Building Your First ML Pipeline.\"\n"
     ]
    }
   ],
   "source": [
    "message = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "# The workshop the MLOps Community hosted together with Girls in Tech Germany was called \"AI Launchpad: Building Your First Ml Pipeline\" or simply \"Building Your First ML Pipeline\"\n",
    "# We copy paste the info from our Eventbrite event page from the previous workshop and use this as context for the model to retrieve the right info from\n",
    "context = \"\"\"Title: AI Launchpad - Building Your First ML Pipeline: \n",
    "On Wednesday, June 5th, 2024, the MLOps Community Berlin in collaboration with Girls in Tech Germany hosted an interactive workshop for beginners who want to kick start their career in AI/ML. \n",
    "The workshop starts at 18.00h at 42Berlin. \n",
    "\n",
    "🔍 Why Attend?\n",
    "\n",
    "Gain hands-on experience building your first ML pipeline in an agile way\n",
    "Apply the fundamentals of statistical modeling and basic Python\n",
    "Opportunities to improve your portfolio \n",
    "Connect with ML professionals at different levels of seniority\n",
    "\n",
    "\n",
    "✨ The Agenda: \n",
    "\n",
    "6:00 pm - Arrive & Pizza \n",
    "6:30 pm -  Introduction MLOps and GiT\n",
    "6:45 pm - Workshop Introduction\n",
    "7:30 pm - Break\n",
    "7:45 pm - Workshop\n",
    "9:45 pm - Networking\n",
    "\n",
    "\n",
    "🎉 Highlights:\n",
    "\n",
    "Food and drinks provided\n",
    "Engaging discussions and networking opportunities\n",
    "Bring your laptop and get ready to learn!\n",
    "\n",
    "\n",
    "💼 Who Should Attend?\n",
    "\n",
    "Individuals starting their career in Machine Learning or Artificial Intelligence\n",
    "Those looking to transition into the field of AI/ML\n",
    "Anyone interested in contributing to and learning from the ML community\n",
    "Don't miss out on this chance to gain practical AI/ML skills while expanding your professional network! \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " I'm an assistant that operates solely on the data it has been trained on up until 2021, and I don't have real-time or future weather data. Therefore, I can't provide the weather forecast for Berlin on the 10th of December, 2027. For the most accurate and up-to-date weather information, I recommend checking a reliable weather website or application closer to the date.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " On the 10th of December, 2027, the weather in Berlin is expected to be around 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same thing with Berlin weather data!\n",
    "context = \"\"\"\n",
    "The weather in Berlin  December of 2027 will be around 13 degrees Celsius.\n",
    "Specific dates:\n",
    "- 10th of December: 10 degrees Celsius\n",
    "- 15th of December: 15 degrees Celsius\n",
    "- 20th of December: 7 degrees Celsius\n",
    "\"\"\"\n",
    "\n",
    "message = \"What will be the weather in Berlin on the 10th of December of 2027?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Now try it yourself! Can you find some content on the internet (think, for example, recent news articles or very specific, locally relevant information that the LLM normally would not have access to). \n",
    "\n",
    "Play around with it and let the creative juices flow. Can you discover some more use cases for which you can use RAG can help make our LLM smarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " I'm sorry for any confusion, but I don't have any specific information about a person named Fanny or her nickname. Nicknames can be very personal and specific to the individual and their social circle. If you have more context or details, I might be able to help you better.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " Fanny's nickname is Squirtle Earmuff.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Fanny's nickname is squirtle earmuff.\n",
    "\"\"\" \n",
    "\n",
    "message = \"\"\"\n",
    "What is Fanny's nickname?\n",
    "\"\"\" \n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "If you want to make your prompting even more advanced, you can look into Prompt Engineering best practices. <br><br>\n",
    "***Prompt Engineering*** is the process of designing and refining LLM prompts. It involves crafting questions, instructions, or statements that guide the model to produce desired outputs. Effective prompt engineering can significantly enhance the quality, relevance, and accuracy of the responses generated by the model.  \n",
    "\n",
    "Best practices change over time and are somewhat different depending on the LLM type, but general guidelines are:\n",
    "\n",
    "* **Clarity and Specificity:** Clearly state what you want the model to do. Avoid ambiguous language.\n",
    "* **Context and Detail:** Provide sufficient context to guide the model.\n",
    "* **Iterative Refinement:** Experiment with different phrasings and structures to see what works best.\n",
    "* **Constraints and Instructions:** Specify any constraints or formats you want the response to follow.\n",
    "* **Examples and Templates:** Use examples to show the model what kind of response you expect.\n",
    "* **Step-by-Step Reasoning:** Instead of providing a direct answer, aske the model to explain its thought process step-by-step. \n",
    "* **Breaking Down Complex Tasks:** For tasks that require multiple stages of reasoning or involve complex information, breaking them down into smaller, separated steps can lead to better results.\n",
    "\n",
    "You can learn more about prompt engineering [here](https://www.promptingguide.ai/). \n",
    "Some very useful examples of best practices for OpenAI GPT models can be found [here](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! \n",
    "\n",
    "RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, such as PDFs, Google search results, social media posts, and more. With that, we’ve built a simple Q&A RAG.\n",
    "\n",
    "## ===> Now head to notebook 2 on embedddings and how llms undertand language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".virtualenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
